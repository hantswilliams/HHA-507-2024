# -*- coding: utf-8 -*-
"""NHANES_2022_2023.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gdQrHgMkfBTS412v1Ch-XaYyCDBmNDJy
"""



"""# National Health and Nutrition Examination Survey (NHANES; 2022-23)

**About**: The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. NHANES is a major program of the National Center for Health Statistics (NCHS). NCHS is part of the Centers for Disease Control and Prevention (CDC) and has the responsibility for producing vital and health statistics for the Nation.

Link: [2022-23 NHINES data](https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?Cycle=2021-2023)

---

# Data files:


- **Demographics**: https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&Cycle=2021-2023
  - File: `DEMO_L.XPT`
  - Data dictionary: https://wwwn.cdc.gov/Nchs/Nhanes/2021-2022/DEMO_L.htm
- **Survey: Hospital Utilization - Access to care**: https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Questionnaire&Cycle=2021-2023
  - File: `HUQ_L.XPT`
  - Data dictionary: https://wwwn.cdc.gov/Nchs/Nhanes/2021-2022/HUQ_L.htm
- **Laboratory Data - Plasma Fasting Glucose**: https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&Cycle=2021-2023
  - File: `GLU_L.XPT`
  - Data dictionary: https://wwwn.cdc.gov/Nchs/Nhanes/2021-2022/GLU_L.htm
- **Laboratory Data - High-Sensitivity C-Reactive Protein**: https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&Cycle=2021-2023
  - File: `HSCRP_L.XPT`
  - Data dictionary: https://wwwn.cdc.gov/Nchs/Nhanes/2021-2022/HSCRP_L.htm
- **Examination Data - Body Measures**: https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&Cycle=2021-2023
  - File: `BMX_L.XPT`
  - Data dictionary: https://wwwn.cdc.gov/Nchs/Nhanes/2021-2022/BMX_L.htm
- **Examination Data - Blood Pressure**:https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&Cycle=2021-2023
  - File:`BPXO_L.XPT`
  - data dictionary: https://wwwn.cdc.gov/Nchs/Nhanes/2021-2022/BPXO_L.htm

"""

# Load Files Demographics

import pandas as pd

demographic_path = 'DEMO_L.XPT'
demo = pd.read_sas(demographic_path, format='xport') #, encoding='latin1')  # Try 'latin1' or 'iso-8859-1' if UTF-8 fails
demo

# Load Files Survey Data - Hospital Utilization and Access to Care

survey_huq_path = 'HUQ_L.XPT'
access = pd.read_sas(survey_huq_path, format='xport')
access

# Load Lab Data: Plasma Fasting Glucose

fasting_glucose_path = 'GLU_L.XPT'
glucose = pd.read_sas(fasting_glucose_path, format='xport')
glucose

# Load Lab Data: C-Reactive Protein

crp_path = 'HSCRP_L.XPT'
crp = pd.read_sas(crp_path, format='xport')
crp

# Load Measurement Data: Body Measures

body_path = 'BMX_L.XPT'
body = pd.read_sas(body_path, format='xport')
body

# Load Measurement Data: Blood Pressure

bp_path = 'BPXO_L.XPT'
bp = pd.read_sas(bp_path, format='xport')
bp

"""# Research Questions

## Create a answerable question

- Based on the data you have, what questions can you attempt to answer
- Non-hypothesis driven question:
  ```
  Does sex, race, and or age influence the likihood of having heart disease?
  ```
- Hypothesis (1-sided) driven driven question:
  ```
  Individuals who are older (age) black (race) males (sex) are more likely to have heart disease compared to other groups.
  ```

## Create a equation

### Variables

$$
dv = iv
$$

- DV: Dependent variable
- IV: Independent variable

### Fill it in

```
DV = ?
IV.1 = ?
IV.2 = ?
IV.3 = ?
```

$$
dv = iv1 + iv2 +iv3
$$

```
dv = heart disease (yes or no)
iv1 = sex (male vs female)
iv2 = race (white vs non-white)
iv3 = age (0-99)
```
"""



"""# Understand Data Types

Review the Data Structure and Measurement Scales

- Ensure you understand the levels of measurement (nominal, ordinal, interval, ratio) for each variable to choose appropriate statistical tests.
  - **Nominal**: Gender: Male, Female, Other
  - **Ordinal**: Pain Scale: None, Mild, Moderate, Severe
  - **Interval**: IQ Scores: The difference between scores is meaningful, but there’s no absolute zero. Interval data have equal intervals, allowing for meaningful differences, but no absolute zero, so ratios are not meaningful.
  - **Ratio**: Height, Weight, Income; Ratio data have a true zero, allowing for meaningful differences, sums, and ratios.


- Some tests are sensitive to the type of data, and using the wrong scale type can lead to invalid results.

# Preliminary Descriptive Statistics and Data Visualization

- Generate summary statistics (mean, median, standard deviation) and visualizations (e.g., histograms, box plots, scatter plots).

- This provides a clear overview of the data's basic characteristics and can reveal potential issues with the data.

# Inferential Statistical Tests

1. **t-Tests**:
  - Used for: assessing difference between two means
  - Assumptions: Normality of differences (paired) or within groups (independent), equal variances for two-sample t-tests.
    - paired: same group at two different times or conditions
    - non-paired (independent): between two different, independent groups
2. **Correlation**:
  - Used for: assessing strength of relationship
    - Pearson: linear relationship, continuous variables
    - Spearman: ranked variables, non-linear
  - Assumptions: Ratio data, linear relationship; normality, equal variances
2. **Chi-Square**:
  - Used for: association between categorical variables
  - Assumptions: Expected frequencies ≥ 5 in each category.
3. **ANOVA**:
  - Used for: assessing mean differences between multiple grows
    - 1way: 1DV, 1IV with min. 2 levels  
    - 2way: 1DV, 2IV each with min. 2 levels
    - 3way: 1DV, 3IV each with min. 2 levels  
  - Assumptions: Normality, equal variances.
4. **Regression**:
  - Used for: prediction, assess strength of relationship
  - Assumptions: Linearity, independence of residuals, homoscedasticity (equal spread of residuals), normal distribution of residuals.

# Assumptions

## 1. Assumption: Data Distribution
- Normality:
  - RATIO data:
    - Tests: Shapiro-Wilk, Kolmogorov-Smirnov.
      - Null Hypothesis (H₀): The data follows a normal distribution.
      - If p-value > 0.05, fail to reject H₀, suggesting the data is likely normal.
    - Visualization: Histograms, Q-Q plots, or skewness/kurtosis values.
      - Skewness: `symmetry` distribution around mean; want close to 0
      - Kurtosis: `shape` - want ~ 3 (mesokurtic)
        - `> 3` = pos kurtosis (leptokurtic) - more data in tails
        - `< 3` = neg kurtosis (platykurtic) - more data around mean
- Shapiro (small data):
  - Python:
    ```python
    from scipy.stats import shapiro
    data = [1.2, 2.5, 3.1, 4.2, 5.1, 5.8, 6.9]
    # Perform Shapiro-Wilk test
    stat, p_value = shapiro(data) # expecting array or list for data
    print(f'Shapiro-Wilk Test Statistic: {stat}, p-value: {p_value}')
    ```
  - R:
    ```r
    # Perform Shapiro-Wilk test
    data <- c(1.2, 2.5, 3.1, 4.2, 5.1, 5.8, 6.9)
    shapiro_test <- shapiro.test(data) # expecting array or list for data
    print(shapiro_test)
    ```
- Kolmogorov (larger data):
  - Python:
    ```python
    from scipy.stats import kstest, norm
    data = [1.2, 2.5, 3.1, 4.2, 5.1, 5.8, 6.9]

    # Standardize data
    data_mean = np.mean(data) # expecting array or list for data
    data_std = np.std(data)
    standardized_data = (data - data_mean) / data_std

    # Perform Kolmogorov-Smirnov test
    stat, p_value = kstest(standardized_data, 'norm')
    print(f'Kolmogorov-Smirnov Test Statistic: {stat}, p-value: {p_value}')
    ```

  - R:
    ```r
    # Standardize the data
    data <- c(1.2, 2.5, 3.1, 4.2, 5.1, 5.8, 6.9)
    standardized_data <- (data - mean(data)) / sd(data) # expecting array or list for data

    # Perform Kolmogorov-Smirnov test
    ks_test <- ks.test(standardized_data, "pnorm")
    print(ks_test)
    ```

## 2. Assumption: Outliers
- RATIO data:
  - Use box plots, Z-scores, or the IQR method to identify and decide whether to keep or remove outliers.
- Non-ratio data:
  - Frequency counts
- Outliers can significantly impact the results, especially in small samples.

- **Ratios: Box Plot**:
  - Interpretation: Points beyond the "whiskers" (typically 1.5 * IQR from the quartiles) are considered outliers.
    - Python:
      ```python
      import matplotlib.pyplot as plt

      data = [1.2, 2.5, 3.1, 4.2, 5.1, 5.8, 6.9, 15]  # adding an extreme value

      # Create a box plot
      plt.boxplot(data) # expecting array or list
      plt.title("Box Plot for Outlier Detection")
      plt.show()
      ```

  - R:
    ```r
    # Create a box plot
    data <- c(1.2, 2.5, 3.1, 4.2, 5.1, 5.8, 6.9, 15)  # adding an extreme value

    boxplot(data, main="Box Plot for Outlier Detection") # expecting array or list
    ```

- **Ratios: Z-Score Method**:
  - Observations with Z-scores beyond a threshold (e.g., ±3) are considered outliers.
    - Python:
      ```python
      import numpy as np

      data = np.array([1.2, 2.5, 3.1, 4.2, 5.1, 5.8, 6.9, 15])  # adding an extreme value

      # Calculate Z-scores
      z_scores = np.abs((data - np.mean(data)) / np.std(data))

      # Identify outliers (Z-score threshold of 3)
      outliers = data[z_scores > 3]
      print(f'Outliers based on Z-scores: {outliers}')
      ```

    - R:
      ```r
      data <- c(1.2, 2.5, 3.1, 4.2, 5.1, 5.8, 6.9, 15)  # adding an extreme value

      # Calculate Z-scores
      z_scores <- abs((data - mean(data)) / sd(data))

      # Identify outliers (Z-score threshold of 3)
      outliers <- data[z_scores > 3]
      print(outliers)
      ```

- **Interquartile range (IQR): Interpretation: Values below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR are outliers**.
  - Python:
    ```python
      # Calculate IQR
      Q1 = np.percentile(data, 25)
      Q3 = np.percentile(data, 75)
      IQR = Q3 - Q1

      # Define bounds for outliers
      lower_bound = Q1 - 1.5 * IQR
      upper_bound = Q3 + 1.5 * IQR

      # Identify outliers
      outliers = data[(data < lower_bound) | (data > upper_bound)]
      print(f'Outliers based on IQR: {outliers}')
      ```

  - R:
    ```r
      # Calculate IQR
      Q1 <- quantile(data, 0.25)
      Q3 <- quantile(data, 0.75)
      IQR <- Q3 - Q1

      # Define bounds for outliers
      lower_bound <- Q1 - 1.5 * IQR
      upper_bound <- Q3 + 1.5 * IQR

      # Identify outliers
      outliers <- data[data < lower_bound | data > upper_bound]
      print(outliers)
    ```

## 3. Assumption: Variation (Homogeneity of Variance)
- RATIO data:
  - Homogeneity of Variance: Many inferential tests assume equal variances between groups - e.g., assume that groups have similar levels of variation across the value
    - Tests: Levene’s test, Bartlett’s test.
      - *Levene’s Test*: More robust and preferred if data are not normally distributed.
      - *Bartlett’s Test*: Assumes normality, making it suitable for normally distributed data.
    - Null Hypothesis (H₀): The variances across groups are equal.
    - If p-value > 0.05, fail to reject H₀, suggesting equal variances.

- **Levene**:
  - Python
    ```python
      from scipy.stats import levene

      # Example data for two groups
      group1 = [1.2, 2.5, 3.1, 4.2, 5.1]
      group2 = [2.1, 2.9, 3.5, 4.7, 5.8]

      # Perform Levene’s test
      stat, p_value = levene(group1, group2)
      print(f"Levene's Test Statistic: {stat}, p-value: {p_value}")

      # Interpretation
      if p_value > 0.05:
          print("Variances are likely equal.")
      else:
          print("Variances are likely not equal.")
    ```

  - R:
    ```r
    # Load the car package for Levene's test
    install.packages("car")  # Run this line if 'car' package is not installed
    library(car)

    # Example data for two groups
    group1 <- c(1.2, 2.5, 3.1, 4.2, 5.1)
    group2 <- c(2.1, 2.9, 3.5, 4.7, 5.8)

    # Perform Levene’s test
    levene_test <- leveneTest(c(group1, group2) ~ rep(1:2, each = length(group1)))
    print(levene_test)
    ```

- **Bartlett**:
  - Python
    ```python
    from scipy.stats import bartlett

    # Example data for two groups
    group1 = [1.2, 2.5, 3.1, 4.2, 5.1]
    group2 = [2.1, 2.9, 3.5, 4.7, 5.8]

    # Perform Bartlett’s test
    stat, p_value = bartlett(group1, group2)
    print(f"Bartlett's Test Statistic: {stat}, p-value: {p_value}")

    # Interpretation
    if p_value > 0.05:
        print("Variances are likely equal.")
    else:
        print("Variances are likely not equal.")
    ```

  - R:
    ```r
    # Example data for two groups
    group1 <- c(1.2, 2.5, 3.1, 4.2, 5.1)
    group2 <- c(2.1, 2.9, 3.5, 4.7, 5.8)

    # Perform Bartlett’s test
    bartlett_test <- bartlett.test(list(group1, group2))
    print(bartlett_test)
    ```

## 4. Assumption: Linearity (correlation, linearity)
- For standard LR, relationships between variables must be linear.
  - assumes that the relationship between the independent and dependent variables is linear
- Use scatter plots or correlation coefficients to assess linearity between variables.
  - *Scatter Plots*: Plot each independent variable against the dependent variable to visually assess whether the data points form a straight line pattern.
  - *Correlation Coefficients*: Calculate Pearson or Spearman correlation coefficients to quantify the strength and direction of the relationship. Strong correlations often indicate linear relationships.

- **Scatter plots**:
  - Python
    ```python
    import matplotlib.pyplot as plt

    # Example data
    x = [1, 2, 3, 4, 5]
    y = [2.1, 4.2, 5.8, 8.1, 9.9]

    # Create a scatter plot
    plt.scatter(x, y)
    plt.xlabel("Independent Variable")
    plt.ylabel("Dependent Variable")
    plt.title("Scatter Plot for Linearity Check")
    plt.show()
    ```

  - R
    ```r
    # Example data
    x <- c(1, 2, 3, 4, 5)
    y <- c(2.1, 4.2, 5.8, 8.1, 9.9)

    # Create a scatter plot
    plot(x, y, xlab = "Independent Variable", ylab = "Dependent Variable", main = "Scatter Plot for Linearity Check")
    ```


- **Correlation Coefficients**:
  - Python
    ```python
    from scipy.stats import pearsonr

    # Calculate Pearson correlation coefficient
    corr, p_value = pearsonr(x, y)
    print(f'Pearson Correlation Coefficient: {corr}, p-value: {p_value}')

    # Interpretation
    if abs(corr) > 0.7:
        print("Strong linear relationship.")
    else:
        print("Weak or non-linear relationship.")
    ```

  - R
    ```
    # Calculate Pearson correlation coefficient
    corr <- cor(x, y, method = "pearson")
    print(paste("Pearson Correlation Coefficient:", corr))

    # Interpretation
    if (abs(corr) > 0.7) {
        cat("Strong linear relationship.\n")
    } else {
        cat("Weak or non-linear relationship.\n")
    }
    ```

## 5. Assumption: Multicollinearity (regression)
- If your analysis involves multiple predictors, test for multicollinearity to avoid redundancy among independent variables.
  - Multicollinearity occurs when two or more independent variables are highly correlated, meaning they provide redundant information.
  - This can inflate standard errors, making it difficult to determine the individual effect of each predictor on the dependent variable.
- Methods: Variance Inflation Factor (VIF) or examining correlation matrices.
  - Variance Inflation Factor (VIF): Measures how much the variance of a regression coefficient is inflated due to multicollinearity. A VIF above 5 (or sometimes 10) typically indicates high multicollinearity.
  - Correlation Matrix: Shows pairwise correlations between variables. High correlations (e.g., > 0.7) suggest multicollinearity.

- **VIF**:
  - Python
    ```python
    import pandas as pd
    from statsmodels.stats.outliers_influence import variance_inflation_factor
    from statsmodels.tools.tools import add_constant

    # Example data
    data = pd.DataFrame({
        'x1': [1, 2, 3, 4, 5],
        'x2': [2, 4, 6, 8, 10],
        'x3': [5, 7, 9, 11, 13]
    })

    # Add a constant to the model (required for VIF calculation)
    data_with_const = add_constant(data)

    # Calculate VIF for each feature
    vif_data = pd.DataFrame()
    vif_data["Variable"] = data_with_const.columns
    vif_data["VIF"] = [variance_inflation_factor(data_with_const.values, i) for i in range(data_with_const.shape[1])]
    print(vif_data)
    ```

  - R
    ```r
    # Load necessary package
    install.packages("car")  # Run this line if 'car' package is not installed
    library(car)

    # Example data
    data <- data.frame(
        x1 = c(1, 2, 3, 4, 5),
        x2 = c(2, 4, 6, 8, 10),
        x3 = c(5, 7, 9, 11, 13)
    )

    # Fit a linear model
    model <- lm(x1 ~ x2 + x3, data = data)

    # Calculate VIF
    vif_values <- vif(model)
    print(vif_values)
    ```

- **Correlation Matrix**:
  - Python
    ```python
    import seaborn as sns
    import matplotlib.pyplot as plt

    # Calculate correlation matrix
    corr_matrix = data.corr()

    # Display the correlation matrix
    print(corr_matrix)

    # Visualize correlation matrix as a heatmap
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
    plt.title("Correlation Matrix")
    plt.show()
    ```

  - R
    ```r
    # Calculate correlation matrix
    corr_matrix <- cor(data)
    print(corr_matrix)

    # Visualize correlation matrix
    install.packages("corrplot")
    library(corrplot)
    corrplot(corr_matrix, method="circle", title="Correlation Matrix")
    ```

# Statistical Test Code: Python and R

## T-tests

### Used for:
- Assessing the difference between two means.
- Assumptions:
  - Normality: The data should be normally distributed within each group (independent) or the differences should be normally distributed (paired).
  - Equal Variances: For the independent t-test, the variances in each group should be similar.
- Types:
  - Paired (Compares means from the same group at two different times or conditions)
  - Non-paired (Compares means between two different, independent groups.)


- **Paired(dependent):**

  - Python:

    ```python
    from scipy.stats import ttest_rel

    # Example data
    group1 = [2.1, 2.5, 2.9, 3.1, 3.8]  # Before treatment
    group2 = [2.3, 2.8, 3.2, 3.3, 4.0]  # After treatment

    # Perform paired t-test
    stat, p_value = ttest_rel(group1, group2)
    print(f"Paired t-Test Statistic: {stat}, p-value: {p_value}")

    # Interpretation
    if p_value > 0.05:
        print("No significant difference between the two conditions.")
    else:
        print("Significant difference between the two conditions.")
    ```

  - R:

    ```r
    # Example data
    group1 <- c(2.1, 2.5, 2.9, 3.1, 3.8)  # Before treatment
    group2 <- c(2.3, 2.8, 3.2, 3.3, 4.0)  # After treatment

    # Perform paired t-test
    paired_test <- t.test(group1, group2, paired = TRUE)
    print(paired_test)

    # Interpretation
    if (paired_test$p.value > 0.05) {
        cat("No significant difference between the two conditions.\n")
    } else {
        cat("Significant difference between the two conditions.\n")
    }
    ```

- **Non-paired (independent)**
  - Python
    ```python
      from scipy.stats import ttest_ind

      # Example data
      group1 = [1.8, 2.4, 3.0, 3.2, 4.1]  # Group A
      group2 = [2.1, 2.7, 3.4, 3.5, 4.3]  # Group B

      # Perform independent t-test
      stat, p_value = ttest_ind(group1, group2, equal_var=True)
      print(f"Independent t-Test Statistic: {stat}, p-value: {p_value}")

      # Interpretation
      if p_value > 0.05:
          print("No significant difference between the groups.")
      else:
          print("Significant difference between the groups.")
    ```

  - R
    ```r
      # Example data
      group1 <- c(1.8, 2.4, 3.0, 3.2, 4.1)  # Group A
      group2 <- c(2.1, 2.7, 3.4, 3.5, 4.3)  # Group B

      # Perform independent t-test
      independent_test <- t.test(group1, group2, var.equal = TRUE)
      print(independent_test)

      # Interpretation
      if (independent_test$p.value > 0.05) {
          cat("No significant difference between the groups.\n")
      } else {
          cat("Significant difference between the groups.\n")
      }
    ```

## Correlations

- Used for: Assessing the strength and direction of the relationship between two variables.
- Types
  - Pearson Correlation: Measures the linear relationship between two continuous variables.
  - Spearman Correlation: Measures the monotonic relationship between ranked or ordinal variables, suitable for non-linear relationships.
- Assumptions:
  - Pearson:
    - Ratio or interval data
    - Linear relationship
    - Normal distribution
    - Homoscedasticity (equal variances)
  - Spearman:
    - Ordinal, interval, or ratio data
    - No assumption of linearity or normality


- **Pearson**
  - Python
    ```python
    from scipy.stats import pearsonr

    # Example data
    x = [1, 2, 3, 4, 5]
    y = [2, 4, 5, 7, 10]

    # Calculate Pearson correlation
    corr, p_value = pearsonr(x, y)
    print(f"Pearson Correlation Coefficient: {corr}, p-value: {p_value}")

    # Interpretation
    if p_value > 0.05:
        print("No significant linear relationship.")
    else:
        print("Significant linear relationship.")
    ```

  - R
    ```r
      # Example data
      x <- c(1, 2, 3, 4, 5)
      y <- c(2, 4, 5, 7, 10)

      # Calculate Pearson correlation
      pearson_corr <- cor.test(x, y, method = "pearson")
      print(pearson_corr)

      # Interpretation
      if (pearson_corr$p.value > 0.05) {
          cat("No significant linear relationship.\n")
      } else {
          cat("Significant linear relationship.\n")
      }
    ```

- **Spearman**
  - Python
    ```python
    from scipy.stats import spearmanr
    # Example data
    x = [1, 2, 3, 4, 5]
    y = [2, 4, 5, 7, 10]

    # Calculate Spearman correlation
    corr, p_value = spearmanr(x, y)
    print(f"Spearman Correlation Coefficient: {corr}, p-value: {p_value}")

    # Interpretation
    if p_value > 0.05:
        print("No significant monotonic relationship.")
    else:
        print("Significant monotonic relationship.")
    ```

  - R
    ```r
    # Example data
    x <- c(1, 2, 3, 4, 5)
    y <- c(2, 4, 5, 7, 10)

    # Calculate Spearman correlation
    spearman_corr <- cor.test(x, y, method = "spearman")
    print(spearman_corr)

    # Interpretation
    if (spearman_corr$p.value > 0.05) {
        cat("No significant monotonic relationship.\n")
    } else {
        cat("Significant monotonic relationship.\n")
    }
    ```

## Chi-Square Test

- Used for: Determining whether there is an association between **two categorical variables**.
- Assumptions:
- The expected frequency in each category should be ≥ 5.
- If expected counts are low, consider combining categories or using an alternative test like Fisher’s Exact Test for smaller sample sizes.


- Python:
  ```python
  import pandas as pd
  from scipy.stats import chi2_contingency

  # Example data: DataFrame with 'sex' and 'ED_visit' columns
  data = {
      'sex': ['Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female'],
      'ED_visit': ['Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'No']
  }

  # Create a DataFrame
  df = pd.DataFrame(data)

  # Create a contingency table
  contingency_table = pd.crosstab(df['sex'], df['ED_visit'])
  print("Contingency Table:")
  print(contingency_table)

  # Perform Chi-Square test
  stat, p_value, dof, expected = chi2_contingency(contingency_table)
  print(f"\nChi-Square Test Statistic: {stat}, p-value: {p_value}")
  print("Expected Frequencies:")
  print(expected)

  # Interpretation
  if p_value > 0.05:
      print("\nNo significant association between sex and ED visit.")
  else:
      print("\nSignificant association between sex and ED visit.")

  ```

- R:
  ```r
    # Example data: DataFrame with 'sex' and 'ED_visit' columns
    sex <- c('Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female')
    ED_visit <- c('Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'No')

    # Create a data frame
    df <- data.frame(sex, ED_visit)

    # Create a contingency table
    contingency_table <- table(df$sex, df$ED_visit)
    print("Contingency Table:")
    print(contingency_table)

    # Perform Chi-Square test
    chi_square_test <- chisq.test(contingency_table)
    print(chi_square_test)

    # Expected frequencies
    print("Expected Frequencies:")
    print(chi_square_test$expected)

    # Interpretation
    if (chi_square_test$p.value > 0.05) {
        cat("\nNo significant association between sex and ED visit.\n")
    } else {
        cat("\nSignificant association between sex and ED visit.\n")
    }

  ```

## ANOVA

- Used for: Assessing mean differences between groups.
  - **1-way ANOVA**: Compares the means of a single dependent variable (DV) across multiple levels of one independent variable (IV).
    - *Example: Testing the mean difference in blood pressure across different age groups.*
  - **2-way ANOVA**: Compares the means of a single dependent variable across multiple levels of two independent variables, allowing you to assess interaction effects.
    - *Example: Testing the mean difference in blood pressure across different age groups and by gender.*
- Assumptions:
  - Normality: The dependent variable should be normally distributed within each group.
  - Equal Variances: The variance across groups should be similar (homogeneity of variance).
  Independence: Observations should be independent of each other.


- 1-way:
  - Python
    ```python
    import pandas as pd
    from scipy.stats import f_oneway

    # Example data
    data = pd.DataFrame({
        'group': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],
        'values': [20, 21, 19, 30, 29, 31, 40, 39, 41]
    })

    # Group the data
    groups = data.groupby('group')['values'].apply(list)

    # Perform 1-way ANOVA
    stat, p_value = f_oneway(*groups)
    print(f"1-Way ANOVA Test Statistic: {stat}, p-value: {p_value}")

    # Interpretation
    if p_value > 0.05:
        print("No significant difference between group means.")
    else:
        print("Significant difference between group means.")
    ```

  - R
    ```r
    # Example data
    group <- factor(c('A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'))
    values <- c(20, 21, 19, 30, 29, 31, 40, 39, 41)

    # Create a data frame
    data <- data.frame(group, values)

    # Perform 1-way ANOVA
    anova_result <- aov(values ~ group, data = data)
    summary(anova_result)

    # Interpretation: Check p-value in the output table under 'Pr(>F)'
    ```

- 2-way
  - Python
  ```python
    import pandas as pd
    import statsmodels.api as sm
    from statsmodels.formula.api import ols

    # Example data
    data = pd.DataFrame({
        'group1': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C'],
        'group2': ['X', 'X', 'Y', 'Y', 'X', 'X', 'Y', 'Y', 'X', 'Y', 'X', 'Y', 'X', 'Y', 'Y'],
        'values': [20, 21, 22, 30, 29, 28, 40, 39, 41, 22, 25, 32, 35, 45, 42]
    })

    # Perform 2-way ANOVA
    model = ols('values ~ C(group1) + C(group2) + C(group1):C(group2)', data=data).fit()
    anova_table = sm.stats.anova_lm(model, typ=2)
    print(anova_table)
  ```

  - R
  ```r
    # Example data
    group1 <- factor(c('A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'A', 'A', 'B', 'B', 'C', 'C'))
    group2 <- factor(c('X', 'X', 'Y', 'Y', 'X', 'X', 'Y', 'Y', 'X', 'Y', 'X', 'Y', 'X', 'Y', 'Y'))
    values <- c(20, 21, 22, 30, 29, 28, 40, 39, 41, 22, 25, 32, 35, 45, 42)

    # Create a data frame
    data <- data.frame(group1, group2, values)

    # Perform 2-way ANOVA
    anova_result <- aov(values ~ group1 + group2 + group1:group2, data = data)
    summary(anova_result)

    # Interpretation: Check p-values in the output for main effects and interaction.
  ```

## Regressions (linear)

- Used for: Making predictions and assessing the strength of the relationship between an independent variable (or variables) and a dependent variable.
  - Assumptions:
    - Linearity: The relationship between the independent and dependent variables should be linear.
    - Independence of Residuals: The residuals (errors) should be independent of each other.
    - Homoscedasticity: The residuals should have constant variance (equal spread) across all levels of the independent variable(s).
    - Normality of Residuals: The residuals should be approximately normally distributed.


- Python
  ```python
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import statsmodels.api as sm
  import seaborn as sns
  from sklearn.linear_model import LinearRegression
  from sklearn.model_selection import train_test_split
  from sklearn.metrics import mean_squared_error, r2_score

  # Example data
  data = {
      'X': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
      'Y': [2.1, 4.2, 5.8, 8.1, 9.9, 12.2, 14.1, 15.9, 18.4, 19.7]
  }
  df = pd.DataFrame(data)

  # Split data into independent and dependent variables
  X = df[['X']]
  y = df['Y']

  # Fit the linear regression model
  model = LinearRegression()
  model.fit(X, y)

  # Predictions
  predictions = model.predict(X)

  # Summary statistics
  print(f'Coefficients: {model.coef_}, Intercept: {model.intercept_}')
  print(f'Mean Squared Error: {mean_squared_error(y, predictions)}')
  print(f'R^2 Score: {r2_score(y, predictions)}')

  # Checking Assumptions
  # Residuals
  residuals = y - predictions

  # Linearity: Scatter plot of X vs Y
  plt.scatter(X, y)
  plt.plot(X, predictions, color='red')
  plt.title('Linearity Check: X vs Y')
  plt.xlabel('X')
  plt.ylabel('Y')
  plt.show()

  # Homoscedasticity: Plot of residuals
  plt.scatter(predictions, residuals)
  plt.axhline(y=0, color='red', linestyle='--')
  plt.title('Homoscedasticity Check: Residuals vs Predictions')
  plt.xlabel('Predicted Values')
  plt.ylabel('Residuals')
  plt.show()

  # Normality of Residuals: Histogram and Q-Q plot
  sns.histplot(residuals, kde=True)
  plt.title('Normality Check: Residuals')
  plt.show()

  sm.qqplot(residuals, line='45')
  plt.title('Q-Q Plot for Residuals')
  plt.show()
  ```


- R
  ```r
  # Example data
  X <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
  Y <- c(2.1, 4.2, 5.8, 8.1, 9.9, 12.2, 14.1, 15.9, 18.4, 19.7)

  # Create data frame
  data <- data.frame(X, Y)

  # Fit the linear regression model
  model <- lm(Y ~ X, data = data)

  # Summary statistics
  summary(model)

  # Checking Assumptions
  # 1. Linearity: Scatter plot with regression line
  plot(data$X, data$Y, main = "Linearity Check: X vs Y", xlab = "X", ylab = "Y")
  abline(model, col = "red")

  # 2. Homoscedasticity: Residuals vs Fitted Values
  plot(model$fitted.values, model$residuals,
      main = "Homoscedasticity Check: Residuals vs Fitted Values",
      xlab = "Fitted Values", ylab = "Residuals")
  abline(h = 0, col = "red", lty = 2)

  # 3. Normality of Residuals: Histogram and Q-Q plot
  hist(model$residuals, main = "Normality Check: Residuals", xlab = "Residuals", prob = TRUE)
  lines(density(model$residuals), col = "blue")

  # Q-Q plot
  qqnorm(model$residuals, main = "Q-Q Plot for Residuals")
  qqline(model$residuals, col = "red")
  ```
"""